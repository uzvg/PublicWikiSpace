## 🧠 大语言模型调参备忘表（Prompt Sampling Parameters Cheatsheet）

| 参数名                               | 推荐范围        | 作用简述                  | 形象比喻         | 用途建议                       |
| --------------------------------- | ----------- | --------------------- | ------------ | -------------------------- |
| **temperature**                   | `0.0 ~ 1.0` | 控制随机性，决定是否冒险选“非最佳答案”  | **撒骰子的热情**   | 高 = 创意写作，低 = 精准问答          |
| **top\_p**                        | `0.8 ~ 1.0` | 控制池子大小，只从累计概率前 P 的词中选 | **池子大小**     | 高 = 发散，低 = 收敛              |
| **top\_k**                        | `20 ~ 100`  | 只从概率最高的前 K 个词中选       | **候选名单长度**   | 限定输出套路，可与 temperature 配合使用 |
| **repetition\_penalty**           | `1.1 ~ 1.5` | 惩罚重复出现的词，防止复读         | **反复说话的惩罚器** | 防止长文本中啰嗦、句式重复              |
| **frequency\_penalty** *(OpenAI)* | `0 ~ 2`     | 降低高频词出现概率             | **话唠词压制器**   | 增加用词多样性                    |
| **presence\_penalty** *(OpenAI)*  | `0 ~ 2`     | 惩罚已出现过的词再出现           | **旧话重提警告器**  | 鼓励探索新话题、新词汇                |

### ✅ 调参小贴士：

* 想要**稳定输出**（如代码、摘要、翻译）：

  * `temperature = 0`，`top_p = 1`，`top_k = 0`（或默认）
* 想要**更有创意**的输出（如写小说、对话生成）：

  * `temperature = 0.8`，`top_p = 0.9`，`top_k = 50`
* 想避免模型“啰嗦复读”：

  * `repetition_penalty = 1.2+`